<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>concrete crack detection using resnet18</title>
  <link rel="stylesheet" href="../styles.css">
  <link rel="stylesheet" href="../css/all.min.css">
  <link rel="icon" type="image/x-icon" href="../MB.png">
</head>
<body>
  <!-- Navigation -->
  <nav>
    <div class="container">
      <a href="../index.html" class="logo">Milad Badeleh</a>
      <button class="menu-toggle" aria-label="Toggle navigation">
        <i class="fas fa-bars"></i>
      </button>
      <ul class="nav-links">
        <li><a href="#about" data-translate="about">About</a></li>
        <li><a href="#research" data-translate="research">Research</a></li>
        <!-- <li><a href="#publications" data-translate="publications">Publications</a></li> -->
        <li><a href="#cv" data-translate="cv">CV</a></li>
        <li><a href="#contact" data-translate="contact">Contact</a></li>
      </ul>
      <!-- Language Selector -->
      <div class="language-selector-wrapper">
        <select id="language-selector">
          <option value="en">English</option>
          <option value="de">Deutsch</option>
          <option value="ru">Русский</option>
          <option value="zh">中文</option>
          <option value="sp">español</option>
          <option value="fr">Français</option>
          <!-- <option value="ar">العربية</option> -->
          <option value="po">Português</option>
          <option value="it">Italiano</option>
          <option value="nl">ِDutch</option>
          <option value="hi">हिंदी</option>
          <option value="ba">বাংলা</option>
        </select>
        <i class="fas fa-globe"></i>
      </div>
      
    </div>
  </nav>

  <!-- Research Content -->
  <section class="research-detail">
    <div class="container">
            <div class="container">
                <h1>ResNet18 Image Classification: A Scientific Overview</h1>
        
                <div class="section">
                    <h2>Introduction</h2>
                    <p>
                        The ResNet18 model is a deep convolutional neural network (CNN) architecture designed for image classification tasks. It is part of the Residual Network (ResNet) family, introduced in the seminal paper <a href="https://arxiv.org/abs/1512.03385" target="_blank">Deep Residual Learning for Image Recognition</a> by Kaiming He et al. in 2015. ResNet18 is particularly notable for its use of <strong>residual connections</strong>, which enable the training of very deep networks by addressing the vanishing gradient problem.
                    </p>
                    <p>
                        This project implements ResNet18 using PyTorch, a popular deep learning framework, to classify images from a custom dataset. The model is trained, validated, and tested, with performance metrics such as loss and accuracy being tracked throughout the process.
                    </p>
                </div>
        
                <div class="section">
                    <h2>Key Concepts</h2>
        
                    <h3>1. Residual Learning</h3>
                    <p>
                        Residual learning is the core innovation behind ResNet. Traditional deep networks often suffer from degradation problems, where adding more layers leads to higher training error. ResNet addresses this by introducing <strong>skip connections</strong> (or shortcut connections) that bypass one or more layers. These connections allow the network to learn <strong>identity mappings</strong>, making it easier to optimize very deep architectures.
                    </p>
                    <p>
                        Mathematically, a residual block can be expressed as:
                        <br>
                        <code>y = F(x, {W_i}) + x</code>
                        <br>
                        where:
                        <ul>
                            <li><code>x</code> is the input to the block,</li>
                            <li><code>F(x, {W_i})</code> represents the transformation applied by the block (e.g., convolutional layers),</li>
                            <li><code>y</code> is the output of the block.</li>
                        </ul>
                        The addition of <code>x</code> ensures that the network can learn residual functions (<code>F</code>) rather than direct mappings, which simplifies optimization.
                    </p>
        
                    <h3>2. ResNet18 Architecture</h3>
                    <p>
                        ResNet18 consists of 18 layers, including:
                        <ul>
                            <li><strong>Convolutional Layers</strong>: The initial layers extract low-level features such as edges and textures.</li>
                            <li><strong>Residual Blocks</strong>: Each block contains two convolutional layers with skip connections.</li>
                            <li><strong>Global Average Pooling</strong>: Instead of fully connected layers, ResNet uses global average pooling to reduce spatial dimensions to 1x1, followed by a fully connected layer for classification.</li>
                            <li><strong>Pretrained Weights</strong>: The model can be initialized with weights pretrained on the ImageNet dataset, enabling transfer learning for custom tasks.</li>
                        </ul>
                        The architecture is lightweight compared to deeper variants like ResNet50 or ResNet101, making it suitable for tasks with limited computational resources.
                    </p>
                </div>
        
                <div class="section">
                    <h2>Implementation Details</h2>
        
                    <h3>1. Data Preprocessing</h3>
                    <p>
                        The input images are preprocessed using the following transformations:
                        <ul>
                            <li><strong>Resizing</strong>: Images are resized to <code>224 × 224</code> pixels, the standard input size for ResNet.</li>
                            <li><strong>Normalization</strong>: Pixel values are normalized using the mean and standard deviation of the ImageNet dataset:
                                <br>
                                <code>mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225]</code>
                            </li>
                        </ul>
                    </p>
        
                    <h3>2. Model Customization</h3>
                    <p>
                        The final fully connected layer of ResNet18 is replaced to match the number of classes in the custom dataset. This allows the model to output predictions specific to the task.
                        <br>
                        <code>model.fc = nn.Linear(model.fc.in_features, num_classes)</code>
                    </p>
        
                    <h3>3. Training and Optimization</h3>
                    <p>
                        The model is trained using the <strong>Adam optimizer</strong> and <strong>Cross-Entropy Loss</strong>, which is standard for classification tasks. The training process includes:
                        <ul>
                            <li><strong>Forward Pass</strong>: Compute predictions and loss.</li>
                            <li><strong>Backward Pass</strong>: Update model weights using gradient descent.</li>
                            <li><strong>Validation</strong>: Evaluate the model on a validation set to monitor generalization performance.</li>
                        </ul>
                    </p>
        
                    <h3>4. Evaluation</h3>
                    <p>
                        After training, the model is evaluated on a separate test set to measure its performance. Metrics such as <strong>loss</strong> and <strong>accuracy</strong> are reported.
                    </p>
                </div>
        
                <div class="section">
                    <h2>Results</h2>
                    <p>
                        The model's performance is quantified using the following metrics:
                        <ul>
                            <li><strong>Training Loss</strong>: Measures how well the model fits the training data.</li>
                            <li><strong>Validation Loss</strong>: Indicates the model's ability to generalize to unseen data.</li>
                            <li><strong>Accuracy</strong>: The percentage of correctly classified images.</li>
                        </ul>
                        These metrics are tracked during training and displayed in real-time using progress bars.
                    </p>
                </div>
        
                <div class="section">
                    <h2>Applications</h2>
                    <p>
                        ResNet18 is widely used in various domains, including:
                        <ul>
                            <li><strong>Medical Imaging</strong>: Classifying diseases from X-rays or MRIs.</li>
                            <li><strong>Autonomous Vehicles</strong>: Recognizing objects in real-time.</li>
                            <li><strong>Agriculture</strong>: Identifying plant diseases from leaf images.</li>
                            <li><strong>Retail</strong>: Classifying products for inventory management.</li>
                        </ul>
                    </p>
                </div>

    <div class="container">
        <h2>Results</h2>

        <div class="section">
            <p>
                The ResNet18 model was trained for <strong>10 epochs</strong> on a custom dataset, achieving excellent performance in both training and validation phases. Below is a summary of the results:
            </p>
        </div>

        <div class="section">
            <h3>Training Performance</h3>
            <ul>
                <li><strong>Final Training Loss</strong>: <code>0.0080</code></li>
                <li><strong>Final Training Accuracy</strong>: <code>99.77%</code></li>
            </ul>
            <p>
                The model consistently achieved high accuracy (<code>>99%</code>) throughout the training process, indicating strong learning capabilities.
            </p>
        </div>

        <div class="section">
            <h3>Validation Performance</h3>
            <ul>
                <li><strong>Final Validation Loss</strong>: <code>0.0067</code></li>
                <li><strong>Final Validation Accuracy</strong>: <code>99.83%</code></li>
            </ul>
            <p>
                The validation accuracy remained stable and high, demonstrating the model's ability to generalize well to unseen data.
            </p>
        </div>

        <div class="section">
            <h3>Testing Performance</h3>
            <ul>
                <li><strong>Test Loss</strong>: <code>0.0067</code></li>
                <li><strong>Test Accuracy</strong>: <code>99.79%</code></li>
            </ul>
            <p>
                The model performed exceptionally well on the test set, confirming its robustness and reliability for image classification tasks.
            </p>
        </div>

        <div class="section">
            <h3>Key Observations</h3>
            <ol>
                <li><strong>High Accuracy</strong>: The model achieved <code>>99% accuracy</code> across all phases (training, validation, and testing), indicating excellent performance.</li>
                <li><strong>Low Loss</strong>: Both training and validation losses were consistently low, suggesting effective optimization and minimal overfitting.</li>
                <li><strong>Stability</strong>: The model's performance remained stable across epochs, with no significant fluctuations in accuracy or loss.</li>
            </ol>
        </div>

        <div class="section">
            <h3>Performance Over Epochs</h3>
            <table>
                <thead>
                    <tr>
                        <th>Epoch</th>
                        <th>Train Loss</th>
                        <th>Train Acc</th>
                        <th>Val Loss</th>
                        <th>Val Acc</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>1</td>
                        <td>0.0276</td>
                        <td>99.16%</td>
                        <td>0.0089</td>
                        <td>99.71%</td>
                    </tr>
                    <tr>
                        <td>2</td>
                        <td>0.0145</td>
                        <td>99.62%</td>
                        <td>0.0085</td>
                        <td>99.72%</td>
                    </tr>
                    <tr>
                        <td>3</td>
                        <td>0.0117</td>
                        <td>99.71%</td>
                        <td>0.0071</td>
                        <td>99.83%</td>
                    </tr>
                    <tr>
                        <td>4</td>
                        <td>0.0098</td>
                        <td>99.72%</td>
                        <td>0.0097</td>
                        <td>99.75%</td>
                    </tr>
                    <tr>
                        <td>5</td>
                        <td>0.0102</td>
                        <td>99.68%</td>
                        <td>0.0088</td>
                        <td>99.71%</td>
                    </tr>
                    <tr>
                        <td>6</td>
                        <td>0.0079</td>
                        <td>99.79%</td>
                        <td>0.0059</td>
                        <td>99.83%</td>
                    </tr>
                    <tr>
                        <td>7</td>
                        <td>0.0090</td>
                        <td>99.76%</td>
                        <td>0.0084</td>
                        <td>99.79%</td>
                    </tr>
                    <tr>
                        <td>8</td>
                        <td>0.0069</td>
                        <td>99.84%</td>
                        <td>0.0090</td>
                        <td>99.75%</td>
                    </tr>
                    <tr>
                        <td>9</td>
                        <td>0.0061</td>
                        <td>99.82%</td>
                        <td>0.0083</td>
                        <td>99.79%</td>
                    </tr>
                    <tr>
                        <td>10</td>
                        <td>0.0080</td>
                        <td>99.77%</td>
                        <td>0.0067</td>
                        <td>99.83%</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="section">
            <h3>Conclusion</h3>
            <p>
                The ResNet18 model demonstrated outstanding performance, achieving <code>>99% accuracy</code> on the test set. This makes it highly suitable for real-world image classification tasks. The model's stability, low loss, and high accuracy across all phases highlight its effectiveness and reliability.
            </p>
        </div>
    </div>
</body>
</html>
                <div class="section">
                    <h2>Conclusion</h2>
                    <p>
                        The ResNet18 model is a powerful and efficient architecture for image classification tasks. Its use of residual connections enables the training of deep networks without degradation, making it a popular choice for both research and industry applications. This project demonstrates how to implement and train ResNet18 using PyTorch, providing a foundation for further exploration and customization.
                    </p>
                </div>
        
                <div class="section">
                    <h2>Code Repository</h2>
                    <p>
                        The complete code for this project is available on <a href="https://github.com/miladbadeleh/Concrete-Crack-Detection-Using-ResNet18" target="_blank">GitHub</a>. Feel free to explore, fork, and contribute!
                    </p>
                </div>
        
            </div>
      <a href="../index.html#research" class="btn btn-primary">Back to Research</a>
    </div>
  </section>

 <!-- Footer -->
 <footer>
    <div class="container">
      <p>&copy; 2025 Milad Badeleh. All rights reserved.</p>
      <div class="social-links">
        <!-- <a href="#"><i class="fab fa-linkedin"></i></a> -->
        <!-- <a href="https://www.researchgate.net/profile/Milad-Badeleh"><i class="fab fa-researchgate"></i></a> -->
        <a href="https://github.com/miladbadeleh"><i class="fab fa-github"></i></a>
        <!-- <a href="https://orcid.org/your-orcid-id" target="_blank"><i class="fab fa-orcid"></i></a>
        <a href="https://www.scopus.com/authid/detail.uri?authorId=YOUR_SCOPUS_ID" target="_blank"><i class="fas fa-book"></i></a>
      <a href="https://scholar.google.com/citations?user=YOUR_GOOGLE_SCHOLAR_ID" target="_blank"><i class="fas fa-graduation-cap"></i></a> -->
      </div>
    </div>
  </footer>

  <script src="script.js"></script>
</body>
</html>